{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59f63c81",
   "metadata": {},
   "source": [
    "# 1. Import libraries and load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e189f100",
   "metadata": {
    "id": "e189f100"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import newaxis\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, SimpleRNN, Dropout, Activation\n",
    "from keras import optimizers\n",
    "\n",
    "from attention import Attention\n",
    "from keras.models import Sequential\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e0ae2c98",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "e0ae2c98",
    "outputId": "9545ff1a-8c47-43e9-bfd8-db047c128cd9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1675.16\n",
       "1    1666.96\n",
       "2    1650.96\n",
       "3    1739.60\n",
       "4    1742.60\n",
       "Name: Price, dtype: float64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./ETH.csv\")\n",
    "df1 = df.reset_index()['Price']\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198f40a5",
   "metadata": {},
   "source": [
    "Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3cc9d4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Scaler data\n",
    "scaler=MinMaxScaler(feature_range=(0,1))\n",
    "df1=scaler.fit_transform(np.array(df1).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27afbf0f",
   "metadata": {},
   "source": [
    "# 2. Split the data into training, test and validate  sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6fe79912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (1214, 1)\n",
      "Test shape: (607, 1)\n",
      "Validate shape: (203, 1)\n"
     ]
    }
   ],
   "source": [
    "train_size = int(0.6 * len(df1))\n",
    "test_size = int(0.3 * len(df1))\n",
    "val_size = len(df1) - train_size - test_size\n",
    "\n",
    "train_data = df1[:train_size]\n",
    "test_data = df1[train_size:train_size+test_size]\n",
    "val_data = df1[train_size+test_size:]\n",
    "\n",
    "print(\"Train shape:\", train_data.shape)\n",
    "print(\"Test shape:\", test_data.shape)\n",
    "print(\"Validate shape:\", val_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5LlVg98rCXB6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5LlVg98rCXB6",
    "outputId": "3663b418-d027-4911-b270-c84e4878c049"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (1416, 1)\n",
      "Test shape: (404, 1)\n",
      "Validate shape: (204, 1)\n"
     ]
    }
   ],
   "source": [
    "train_size = int(0.7 * len(df1))\n",
    "test_size = int(0.2 * len(df1))\n",
    "val_size = len(df1) - train_size - test_size\n",
    "\n",
    "train_data = df1[:train_size]\n",
    "test_data = df1[train_size:train_size+test_size]\n",
    "val_data = df1[train_size+test_size:]\n",
    "\n",
    "print(\"Train shape:\", train_data.shape)\n",
    "print(\"Test shape:\", test_data.shape)\n",
    "print(\"Validate shape:\", val_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bc3a0fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (1214, 1)\n",
      "Test shape: (404, 1)\n",
      "Validate shape: (406, 1)\n"
     ]
    }
   ],
   "source": [
    "train_size = int(0.6 * len(df1))\n",
    "test_size = int(0.2 * len(df1))\n",
    "val_size = len(df1) - train_size - test_size\n",
    "\n",
    "train_data = df1[:train_size]\n",
    "test_data = df1[train_size:train_size+test_size]\n",
    "val_data = df1[train_size+test_size:]\n",
    "\n",
    "print(\"Train shape:\", train_data.shape)\n",
    "print(\"Test shape:\", test_data.shape)\n",
    "print(\"Validate shape:\", val_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "RexiMuBnDl-N",
   "metadata": {
    "id": "RexiMuBnDl-N"
   },
   "outputs": [],
   "source": [
    "# 5. HÃ m Create Dataset\n",
    "import numpy\n",
    "# convert an array of values into a dataset matrix\n",
    "def create_dataset(dataset, time_step=1):\n",
    "\tdataX, dataY = [], []\n",
    "\tfor i in range(len(dataset)-time_step-1):\n",
    "\t\ta = dataset[i:(i+time_step), 0]   ###i=0, X=0,1,2,3-----99   Y=100 \n",
    "\t\tdataX.append(a)\n",
    "\t\tdataY.append(dataset[i + time_step, 0])\n",
    "\treturn numpy.array(dataX), numpy.array(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "w9YQkHz8Doi_",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w9YQkHz8Doi_",
    "outputId": "7bcd1a64-a271-468d-ca89-d952281c7c85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "303\n"
     ]
    }
   ],
   "source": [
    "#6. Reshape into X=t,t+1,t+2..t+99 and Y=t+100\n",
    "\n",
    "time_step = 100\n",
    "X_train, y_train = create_dataset(train_data, time_step)\n",
    "X_val, y_val = create_dataset(val_data, time_step)\n",
    "X_test, y_test = create_dataset(test_data, time_step)\n",
    "print(len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "DS0uif1gDtTC",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DS0uif1gDtTC",
    "outputId": "4b9dee41-4edb-4795-f04e-fa826dc7c90e"
   },
   "outputs": [],
   "source": [
    "# 7. Reshape input to be [samples, time steps, features] which is required for LSTM\n",
    "\n",
    "X_train =X_train.reshape(X_train.shape[0],X_train.shape[1] , 1)\n",
    "X_test = X_test.reshape(X_test.shape[0],X_test.shape[1] , 1)\n",
    "X_val = X_val.reshape(X_val.shape[0],X_val.shape[1] , 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd623ffb",
   "metadata": {},
   "source": [
    "# 3. Perform RNN-Attention Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b395615",
   "metadata": {},
   "source": [
    "## 3.1 Build the RNN Attention Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0543b379",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0543b379",
    "outputId": "61cfb4c0-1b2f-4599-c5b4-d6bc96fba78f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " simple_rnn_3 (SimpleRNN)    (None, 100, 100)          10200     \n",
      "                                                                 \n",
      " attention_3 (Attention)     (None, 128)               35600     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 32)                4128      \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 49,961\n",
      "Trainable params: 49,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    SimpleRNN(100, return_sequences=True, input_shape=(time_step, 1)),\n",
    "    Attention(),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='linear')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "34a06337",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "34a06337",
    "outputId": "4722614d-f586-4f0b-8899-4af7c4404aae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "18/18 [==============================] - 5s 101ms/step - loss: 0.0364 - accuracy: 8.9847e-04 - val_loss: 8.0396e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/200\n",
      "18/18 [==============================] - 1s 54ms/step - loss: 0.0015 - accuracy: 8.9847e-04 - val_loss: 1.7243e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/200\n",
      "18/18 [==============================] - 1s 55ms/step - loss: 8.9737e-04 - accuracy: 8.9847e-04 - val_loss: 7.2205e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/200\n",
      "18/18 [==============================] - 1s 50ms/step - loss: 7.2109e-04 - accuracy: 8.9847e-04 - val_loss: 4.4955e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/200\n",
      "18/18 [==============================] - 1s 52ms/step - loss: 6.5775e-04 - accuracy: 8.9847e-04 - val_loss: 2.4307e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/200\n",
      "18/18 [==============================] - 1s 49ms/step - loss: 5.8970e-04 - accuracy: 8.9847e-04 - val_loss: 2.3425e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/200\n",
      "18/18 [==============================] - 1s 49ms/step - loss: 5.7179e-04 - accuracy: 8.9847e-04 - val_loss: 2.7796e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/200\n",
      "18/18 [==============================] - 1s 50ms/step - loss: 5.4473e-04 - accuracy: 8.9847e-04 - val_loss: 1.5862e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/200\n",
      "18/18 [==============================] - 1s 50ms/step - loss: 5.9678e-04 - accuracy: 8.9847e-04 - val_loss: 2.8706e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/200\n",
      "18/18 [==============================] - 1s 50ms/step - loss: 6.1392e-04 - accuracy: 8.9847e-04 - val_loss: 1.3292e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/200\n",
      "18/18 [==============================] - 1s 49ms/step - loss: 6.5076e-04 - accuracy: 8.9847e-04 - val_loss: 1.4693e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/200\n",
      "18/18 [==============================] - 1s 52ms/step - loss: 5.2961e-04 - accuracy: 8.9847e-04 - val_loss: 1.4642e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/200\n",
      "18/18 [==============================] - 1s 47ms/step - loss: 5.2043e-04 - accuracy: 8.9847e-04 - val_loss: 2.1911e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/200\n",
      "18/18 [==============================] - 1s 48ms/step - loss: 5.4510e-04 - accuracy: 8.9847e-04 - val_loss: 1.8208e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/200\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 5.5087e-04 - accuracy: 8.9847e-04 - val_loss: 2.0724e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/200\n",
      "18/18 [==============================] - 1s 43ms/step - loss: 5.6551e-04 - accuracy: 8.9847e-04 - val_loss: 1.6950e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/200\n",
      "18/18 [==============================] - 1s 43ms/step - loss: 5.4138e-04 - accuracy: 8.9847e-04 - val_loss: 1.8172e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/200\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 4.8447e-04 - accuracy: 8.9847e-04 - val_loss: 1.9677e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/200\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 5.0874e-04 - accuracy: 8.9847e-04 - val_loss: 2.5454e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/200\n",
      "18/18 [==============================] - 1s 48ms/step - loss: 5.9977e-04 - accuracy: 8.9847e-04 - val_loss: 1.5375e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 21/200\n",
      "18/18 [==============================] - 1s 47ms/step - loss: 5.0044e-04 - accuracy: 8.9847e-04 - val_loss: 3.7702e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 22/200\n",
      "18/18 [==============================] - 1s 42ms/step - loss: 5.2162e-04 - accuracy: 8.9847e-04 - val_loss: 1.4898e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 23/200\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 5.3465e-04 - accuracy: 8.9847e-04 - val_loss: 2.2239e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 24/200\n",
      "18/18 [==============================] - 1s 49ms/step - loss: 4.6366e-04 - accuracy: 8.9847e-04 - val_loss: 1.3453e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 25/200\n",
      "18/18 [==============================] - 1s 44ms/step - loss: 5.2366e-04 - accuracy: 8.9847e-04 - val_loss: 1.3671e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 26/200\n",
      "18/18 [==============================] - 1s 44ms/step - loss: 4.8608e-04 - accuracy: 8.9847e-04 - val_loss: 1.5793e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 27/200\n",
      "18/18 [==============================] - 1s 44ms/step - loss: 4.5642e-04 - accuracy: 8.9847e-04 - val_loss: 2.9954e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 28/200\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 4.9460e-04 - accuracy: 8.9847e-04 - val_loss: 1.4596e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 29/200\n",
      "18/18 [==============================] - 1s 43ms/step - loss: 5.5089e-04 - accuracy: 8.9847e-04 - val_loss: 1.3054e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 30/200\n",
      "18/18 [==============================] - 1s 50ms/step - loss: 5.9187e-04 - accuracy: 8.9847e-04 - val_loss: 2.1454e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 31/200\n",
      "18/18 [==============================] - 1s 43ms/step - loss: 5.3602e-04 - accuracy: 8.9847e-04 - val_loss: 1.7122e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 32/200\n",
      "18/18 [==============================] - 1s 44ms/step - loss: 4.9375e-04 - accuracy: 8.9847e-04 - val_loss: 1.3115e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 33/200\n",
      "18/18 [==============================] - 1s 54ms/step - loss: 4.8579e-04 - accuracy: 8.9847e-04 - val_loss: 1.9854e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 34/200\n",
      "18/18 [==============================] - 1s 43ms/step - loss: 4.5331e-04 - accuracy: 8.9847e-04 - val_loss: 1.6830e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 35/200\n",
      "18/18 [==============================] - 1s 42ms/step - loss: 4.6102e-04 - accuracy: 8.9847e-04 - val_loss: 1.0583e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 36/200\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 4.5100e-04 - accuracy: 8.9847e-04 - val_loss: 1.2863e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 37/200\n",
      "18/18 [==============================] - 1s 43ms/step - loss: 4.5427e-04 - accuracy: 8.9847e-04 - val_loss: 1.2112e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 38/200\n",
      "18/18 [==============================] - 1s 44ms/step - loss: 4.4418e-04 - accuracy: 8.9847e-04 - val_loss: 2.0055e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 39/200\n",
      "18/18 [==============================] - 1s 53ms/step - loss: 4.6487e-04 - accuracy: 8.9847e-04 - val_loss: 2.0527e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 40/200\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 4.3973e-04 - accuracy: 8.9847e-04 - val_loss: 1.2918e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 41/200\n",
      "18/18 [==============================] - 1s 43ms/step - loss: 5.2045e-04 - accuracy: 8.9847e-04 - val_loss: 1.0036e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 42/200\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 4.6759e-04 - accuracy: 8.9847e-04 - val_loss: 1.3414e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 43/200\n",
      "18/18 [==============================] - 1s 44ms/step - loss: 5.4255e-04 - accuracy: 8.9847e-04 - val_loss: 1.0141e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 44/200\n",
      "18/18 [==============================] - 1s 42ms/step - loss: 4.4746e-04 - accuracy: 8.9847e-04 - val_loss: 1.8989e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 45/200\n",
      "18/18 [==============================] - 1s 41ms/step - loss: 4.3953e-04 - accuracy: 8.9847e-04 - val_loss: 1.2131e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 46/200\n",
      "18/18 [==============================] - 1s 39ms/step - loss: 4.4403e-04 - accuracy: 8.9847e-04 - val_loss: 1.6278e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 47/200\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 4.4786e-04 - accuracy: 8.9847e-04 - val_loss: 1.4298e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 48/200\n",
      "18/18 [==============================] - 1s 42ms/step - loss: 4.3527e-04 - accuracy: 8.9847e-04 - val_loss: 1.5281e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 49/200\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 4.7046e-04 - accuracy: 8.9847e-04 - val_loss: 1.9246e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 50/200\n",
      "18/18 [==============================] - 1s 52ms/step - loss: 5.0462e-04 - accuracy: 8.9847e-04 - val_loss: 2.0501e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 51/200\n",
      "18/18 [==============================] - 1s 52ms/step - loss: 4.6057e-04 - accuracy: 8.9847e-04 - val_loss: 3.1553e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 52/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 1s 47ms/step - loss: 4.6375e-04 - accuracy: 8.9847e-04 - val_loss: 9.7194e-06 - val_accuracy: 0.0000e+00\n",
      "Epoch 53/200\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 4.8709e-04 - accuracy: 8.9847e-04 - val_loss: 1.2617e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 54/200\n",
      "18/18 [==============================] - 1s 51ms/step - loss: 4.7814e-04 - accuracy: 8.9847e-04 - val_loss: 1.0231e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 55/200\n",
      "18/18 [==============================] - 1s 43ms/step - loss: 5.0345e-04 - accuracy: 8.9847e-04 - val_loss: 1.4546e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 56/200\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 4.5503e-04 - accuracy: 8.9847e-04 - val_loss: 1.5436e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 57/200\n",
      "18/18 [==============================] - 1s 43ms/step - loss: 4.3517e-04 - accuracy: 8.9847e-04 - val_loss: 1.5270e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 58/200\n",
      "18/18 [==============================] - 1s 44ms/step - loss: 4.3830e-04 - accuracy: 8.9847e-04 - val_loss: 1.2742e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 59/200\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 5.3995e-04 - accuracy: 8.9847e-04 - val_loss: 2.5047e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 60/200\n",
      "18/18 [==============================] - 1s 47ms/step - loss: 4.6848e-04 - accuracy: 8.9847e-04 - val_loss: 1.0402e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 61/200\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 4.7625e-04 - accuracy: 8.9847e-04 - val_loss: 1.3845e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 62/200\n",
      "18/18 [==============================] - 1s 39ms/step - loss: 4.7598e-04 - accuracy: 8.9847e-04 - val_loss: 2.0811e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 63/200\n",
      "18/18 [==============================] - 1s 43ms/step - loss: 4.4997e-04 - accuracy: 8.9847e-04 - val_loss: 3.1545e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 64/200\n",
      "18/18 [==============================] - 1s 41ms/step - loss: 4.7346e-04 - accuracy: 8.9847e-04 - val_loss: 9.4215e-06 - val_accuracy: 0.0000e+00\n",
      "Epoch 65/200\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 4.2811e-04 - accuracy: 8.9847e-04 - val_loss: 4.1146e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 66/200\n",
      "18/18 [==============================] - 1s 42ms/step - loss: 4.6053e-04 - accuracy: 8.9847e-04 - val_loss: 1.6308e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 67/200\n",
      "18/18 [==============================] - 1s 40ms/step - loss: 4.5459e-04 - accuracy: 8.9847e-04 - val_loss: 1.1477e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 68/200\n",
      "18/18 [==============================] - 1s 42ms/step - loss: 4.1568e-04 - accuracy: 8.9847e-04 - val_loss: 1.5833e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 69/200\n",
      "18/18 [==============================] - 1s 48ms/step - loss: 4.3443e-04 - accuracy: 8.9847e-04 - val_loss: 1.0859e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 70/200\n",
      "18/18 [==============================] - 1s 43ms/step - loss: 4.6099e-04 - accuracy: 8.9847e-04 - val_loss: 1.1768e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 71/200\n",
      "18/18 [==============================] - 1s 44ms/step - loss: 4.2582e-04 - accuracy: 8.9847e-04 - val_loss: 2.1047e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 72/200\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 4.0049e-04 - accuracy: 8.9847e-04 - val_loss: 2.2819e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 73/200\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 4.5830e-04 - accuracy: 8.9847e-04 - val_loss: 7.8023e-06 - val_accuracy: 0.0000e+00\n",
      "Epoch 74/200\n",
      "18/18 [==============================] - 1s 44ms/step - loss: 4.6222e-04 - accuracy: 8.9847e-04 - val_loss: 2.9685e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 75/200\n",
      "18/18 [==============================] - 1s 44ms/step - loss: 4.1248e-04 - accuracy: 8.9847e-04 - val_loss: 1.2103e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 76/200\n",
      "18/18 [==============================] - 1s 47ms/step - loss: 5.3591e-04 - accuracy: 8.9847e-04 - val_loss: 9.7641e-06 - val_accuracy: 0.0000e+00\n",
      "Epoch 77/200\n",
      "18/18 [==============================] - 1s 48ms/step - loss: 4.9697e-04 - accuracy: 8.9847e-04 - val_loss: 3.1486e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 78/200\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 4.7037e-04 - accuracy: 8.9847e-04 - val_loss: 1.6841e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 79/200\n",
      "18/18 [==============================] - 1s 44ms/step - loss: 4.3036e-04 - accuracy: 8.9847e-04 - val_loss: 1.3892e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 80/200\n",
      "18/18 [==============================] - 1s 56ms/step - loss: 4.6124e-04 - accuracy: 8.9847e-04 - val_loss: 1.9907e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 81/200\n",
      "18/18 [==============================] - 1s 41ms/step - loss: 4.3843e-04 - accuracy: 8.9847e-04 - val_loss: 7.9884e-06 - val_accuracy: 0.0000e+00\n",
      "Epoch 82/200\n",
      "18/18 [==============================] - 1s 52ms/step - loss: 4.2131e-04 - accuracy: 8.9847e-04 - val_loss: 2.8489e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 83/200\n",
      "18/18 [==============================] - 1s 51ms/step - loss: 4.1890e-04 - accuracy: 8.9847e-04 - val_loss: 7.5933e-06 - val_accuracy: 0.0000e+00\n",
      "Epoch 84/200\n",
      "18/18 [==============================] - 1s 59ms/step - loss: 4.6541e-04 - accuracy: 8.9847e-04 - val_loss: 1.7120e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 85/200\n",
      "18/18 [==============================] - 1s 49ms/step - loss: 5.1537e-04 - accuracy: 8.9847e-04 - val_loss: 9.3467e-06 - val_accuracy: 0.0000e+00\n",
      "Epoch 86/200\n",
      "18/18 [==============================] - 1s 52ms/step - loss: 4.1994e-04 - accuracy: 8.9847e-04 - val_loss: 8.3849e-06 - val_accuracy: 0.0000e+00\n",
      "Epoch 87/200\n",
      "18/18 [==============================] - 1s 53ms/step - loss: 4.2218e-04 - accuracy: 8.9847e-04 - val_loss: 6.5712e-06 - val_accuracy: 0.0000e+00\n",
      "Epoch 88/200\n",
      "18/18 [==============================] - 1s 50ms/step - loss: 3.9255e-04 - accuracy: 8.9847e-04 - val_loss: 2.5634e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 89/200\n",
      "18/18 [==============================] - 1s 51ms/step - loss: 3.8642e-04 - accuracy: 8.9847e-04 - val_loss: 8.2942e-06 - val_accuracy: 0.0000e+00\n",
      "Epoch 90/200\n",
      "18/18 [==============================] - 1s 47ms/step - loss: 4.1988e-04 - accuracy: 8.9847e-04 - val_loss: 1.1382e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 91/200\n",
      "18/18 [==============================] - 1s 49ms/step - loss: 4.0033e-04 - accuracy: 8.9847e-04 - val_loss: 1.0825e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 92/200\n",
      "18/18 [==============================] - 1s 51ms/step - loss: 3.9998e-04 - accuracy: 8.9847e-04 - val_loss: 9.8521e-06 - val_accuracy: 0.0000e+00\n",
      "Epoch 93/200\n",
      "18/18 [==============================] - 1s 53ms/step - loss: 4.0345e-04 - accuracy: 8.9847e-04 - val_loss: 8.1621e-06 - val_accuracy: 0.0000e+00\n",
      "Epoch 94/200\n",
      "18/18 [==============================] - 1s 58ms/step - loss: 3.9719e-04 - accuracy: 8.9847e-04 - val_loss: 7.9479e-06 - val_accuracy: 0.0000e+00\n",
      "Epoch 95/200\n",
      "18/18 [==============================] - 1s 53ms/step - loss: 4.1424e-04 - accuracy: 8.9847e-04 - val_loss: 7.1999e-06 - val_accuracy: 0.0000e+00\n",
      "Epoch 96/200\n",
      "18/18 [==============================] - 1s 57ms/step - loss: 4.2359e-04 - accuracy: 8.9847e-04 - val_loss: 1.1148e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 97/200\n",
      "18/18 [==============================] - 1s 52ms/step - loss: 4.2486e-04 - accuracy: 8.9847e-04 - val_loss: 7.4782e-06 - val_accuracy: 0.0000e+00\n",
      "Epoch 98/200\n",
      "18/18 [==============================] - 1s 53ms/step - loss: 4.3960e-04 - accuracy: 8.9847e-04 - val_loss: 6.0220e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 99/200\n",
      "18/18 [==============================] - 1s 56ms/step - loss: 4.3711e-04 - accuracy: 8.9847e-04 - val_loss: 3.0104e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 100/200\n",
      "18/18 [==============================] - 1s 53ms/step - loss: 4.5323e-04 - accuracy: 8.9847e-04 - val_loss: 1.4143e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 101/200\n",
      "18/18 [==============================] - 1s 77ms/step - loss: 5.0619e-04 - accuracy: 8.9847e-04 - val_loss: 1.1064e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 102/200\n",
      "18/18 [==============================] - 1s 57ms/step - loss: 4.4846e-04 - accuracy: 8.9847e-04 - val_loss: 1.0340e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 103/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 1s 62ms/step - loss: 4.5067e-04 - accuracy: 8.9847e-04 - val_loss: 1.7479e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 104/200\n",
      "18/18 [==============================] - 1s 57ms/step - loss: 4.3901e-04 - accuracy: 8.9847e-04 - val_loss: 1.5854e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 105/200\n",
      "18/18 [==============================] - 1s 58ms/step - loss: 4.1152e-04 - accuracy: 8.9847e-04 - val_loss: 1.9083e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 106/200\n",
      "18/18 [==============================] - 1s 54ms/step - loss: 4.1079e-04 - accuracy: 8.9847e-04 - val_loss: 1.5134e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 107/200\n",
      "18/18 [==============================] - 1s 55ms/step - loss: 4.0530e-04 - accuracy: 8.9847e-04 - val_loss: 1.9243e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 108/200\n",
      "18/18 [==============================] - 1s 54ms/step - loss: 3.9489e-04 - accuracy: 8.9847e-04 - val_loss: 7.8550e-06 - val_accuracy: 0.0000e+00\n",
      "Epoch 109/200\n",
      "18/18 [==============================] - 1s 57ms/step - loss: 3.8592e-04 - accuracy: 8.9847e-04 - val_loss: 1.1882e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 110/200\n",
      "18/18 [==============================] - 1s 67ms/step - loss: 3.7771e-04 - accuracy: 8.9847e-04 - val_loss: 1.5004e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 111/200\n",
      "18/18 [==============================] - 1s 56ms/step - loss: 4.0932e-04 - accuracy: 8.9847e-04 - val_loss: 3.4032e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 112/200\n",
      "18/18 [==============================] - 1s 65ms/step - loss: 3.9486e-04 - accuracy: 8.9847e-04 - val_loss: 2.7382e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 113/200\n",
      "18/18 [==============================] - 1s 53ms/step - loss: 4.0635e-04 - accuracy: 8.9847e-04 - val_loss: 1.3405e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 114/200\n",
      "18/18 [==============================] - 1s 54ms/step - loss: 4.6366e-04 - accuracy: 8.9847e-04 - val_loss: 9.8045e-06 - val_accuracy: 0.0000e+00\n",
      "Epoch 115/200\n",
      "18/18 [==============================] - 1s 57ms/step - loss: 4.1946e-04 - accuracy: 8.9847e-04 - val_loss: 7.4212e-06 - val_accuracy: 0.0000e+00\n",
      "Epoch 116/200\n",
      "18/18 [==============================] - 1s 63ms/step - loss: 4.4087e-04 - accuracy: 8.9847e-04 - val_loss: 1.1808e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 117/200\n",
      "18/18 [==============================] - 1s 55ms/step - loss: 4.2180e-04 - accuracy: 8.9847e-04 - val_loss: 2.3715e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 118/200\n",
      "18/18 [==============================] - 1s 54ms/step - loss: 3.9292e-04 - accuracy: 8.9847e-04 - val_loss: 1.0807e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 119/200\n",
      "18/18 [==============================] - 1s 60ms/step - loss: 5.9803e-04 - accuracy: 8.9847e-04 - val_loss: 9.1878e-06 - val_accuracy: 0.0000e+00\n",
      "Epoch 120/200\n",
      "18/18 [==============================] - 1s 55ms/step - loss: 5.0488e-04 - accuracy: 8.9847e-04 - val_loss: 8.3424e-06 - val_accuracy: 0.0000e+00\n",
      "Epoch 121/200\n",
      "18/18 [==============================] - 1s 56ms/step - loss: 3.8693e-04 - accuracy: 8.9847e-04 - val_loss: 1.6664e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 122/200\n",
      "18/18 [==============================] - 1s 63ms/step - loss: 4.0202e-04 - accuracy: 8.9847e-04 - val_loss: 1.3029e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 123/200\n",
      "18/18 [==============================] - 1s 59ms/step - loss: 3.8267e-04 - accuracy: 8.9847e-04 - val_loss: 6.2008e-06 - val_accuracy: 0.0000e+00\n",
      "Epoch 124/200\n",
      "18/18 [==============================] - 1s 64ms/step - loss: 4.1053e-04 - accuracy: 8.9847e-04 - val_loss: 1.1176e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 125/200\n",
      "18/18 [==============================] - 1s 62ms/step - loss: 5.0650e-04 - accuracy: 8.9847e-04 - val_loss: 7.8848e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 126/200\n",
      "18/18 [==============================] - 1s 53ms/step - loss: 6.3165e-04 - accuracy: 8.9847e-04 - val_loss: 1.3598e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 127/200\n",
      "18/18 [==============================] - 1s 54ms/step - loss: 5.3162e-04 - accuracy: 8.9847e-04 - val_loss: 1.1142e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 128/200\n",
      "18/18 [==============================] - 1s 52ms/step - loss: 3.9950e-04 - accuracy: 8.9847e-04 - val_loss: 3.6670e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 129/200\n",
      "18/18 [==============================] - 1s 54ms/step - loss: 3.9643e-04 - accuracy: 8.9847e-04 - val_loss: 1.5307e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 130/200\n",
      "18/18 [==============================] - 1s 52ms/step - loss: 4.2287e-04 - accuracy: 8.9847e-04 - val_loss: 3.0446e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 131/200\n",
      "18/18 [==============================] - 1s 56ms/step - loss: 4.2636e-04 - accuracy: 8.9847e-04 - val_loss: 2.3462e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 132/200\n",
      "18/18 [==============================] - 1s 53ms/step - loss: 5.0304e-04 - accuracy: 8.9847e-04 - val_loss: 1.5911e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 133/200\n",
      "18/18 [==============================] - 1s 57ms/step - loss: 4.1719e-04 - accuracy: 8.9847e-04 - val_loss: 1.5405e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 134/200\n",
      "18/18 [==============================] - 1s 55ms/step - loss: 3.9374e-04 - accuracy: 8.9847e-04 - val_loss: 1.6117e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 135/200\n",
      "18/18 [==============================] - 1s 54ms/step - loss: 3.9654e-04 - accuracy: 8.9847e-04 - val_loss: 1.1418e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 136/200\n",
      "18/18 [==============================] - 1s 55ms/step - loss: 4.1599e-04 - accuracy: 8.9847e-04 - val_loss: 1.7738e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 137/200\n",
      "18/18 [==============================] - 1s 61ms/step - loss: 3.6213e-04 - accuracy: 8.9847e-04 - val_loss: 1.0569e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 138/200\n",
      "18/18 [==============================] - 1s 56ms/step - loss: 3.5204e-04 - accuracy: 8.9847e-04 - val_loss: 8.2279e-06 - val_accuracy: 0.0000e+00\n",
      "Epoch 139/200\n",
      "18/18 [==============================] - 1s 68ms/step - loss: 3.9053e-04 - accuracy: 8.9847e-04 - val_loss: 8.6871e-06 - val_accuracy: 0.0000e+00\n",
      "Epoch 140/200\n",
      "18/18 [==============================] - 1s 63ms/step - loss: 3.7471e-04 - accuracy: 8.9847e-04 - val_loss: 3.2909e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 141/200\n",
      "18/18 [==============================] - 1s 70ms/step - loss: 4.1835e-04 - accuracy: 8.9847e-04 - val_loss: 1.6386e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 142/200\n",
      "18/18 [==============================] - 1s 52ms/step - loss: 3.9631e-04 - accuracy: 8.9847e-04 - val_loss: 1.2409e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 143/200\n",
      "18/18 [==============================] - 1s 55ms/step - loss: 4.1477e-04 - accuracy: 8.9847e-04 - val_loss: 2.5702e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 144/200\n",
      "18/18 [==============================] - 1s 56ms/step - loss: 3.8932e-04 - accuracy: 8.9847e-04 - val_loss: 1.0757e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 145/200\n",
      "18/18 [==============================] - 1s 77ms/step - loss: 3.6339e-04 - accuracy: 8.9847e-04 - val_loss: 9.9565e-06 - val_accuracy: 0.0000e+00\n",
      "Epoch 146/200\n",
      "18/18 [==============================] - 1s 61ms/step - loss: 3.6749e-04 - accuracy: 8.9847e-04 - val_loss: 8.5157e-06 - val_accuracy: 0.0000e+00\n",
      "Epoch 147/200\n",
      "18/18 [==============================] - 1s 63ms/step - loss: 3.5266e-04 - accuracy: 8.9847e-04 - val_loss: 1.7444e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 148/200\n",
      "18/18 [==============================] - 1s 66ms/step - loss: 3.6751e-04 - accuracy: 8.9847e-04 - val_loss: 1.1892e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 149/200\n",
      "18/18 [==============================] - 2s 90ms/step - loss: 3.8477e-04 - accuracy: 8.9847e-04 - val_loss: 2.9173e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 150/200\n",
      "18/18 [==============================] - 1s 82ms/step - loss: 4.1990e-04 - accuracy: 8.9847e-04 - val_loss: 8.8683e-06 - val_accuracy: 0.0000e+00\n",
      "Epoch 151/200\n",
      "18/18 [==============================] - 1s 78ms/step - loss: 3.7330e-04 - accuracy: 8.9847e-04 - val_loss: 8.3085e-06 - val_accuracy: 0.0000e+00\n",
      "Epoch 152/200\n",
      "18/18 [==============================] - 2s 91ms/step - loss: 3.8094e-04 - accuracy: 8.9847e-04 - val_loss: 1.1798e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 153/200\n",
      "18/18 [==============================] - 2s 96ms/step - loss: 3.3879e-04 - accuracy: 8.9847e-04 - val_loss: 1.0723e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 154/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 2s 88ms/step - loss: 3.7443e-04 - accuracy: 8.9847e-04 - val_loss: 5.5716e-06 - val_accuracy: 0.0000e+00\n",
      "Epoch 155/200\n",
      "18/18 [==============================] - 1s 80ms/step - loss: 3.8762e-04 - accuracy: 8.9847e-04 - val_loss: 2.7197e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 156/200\n",
      "18/18 [==============================] - 2s 88ms/step - loss: 3.4873e-04 - accuracy: 8.9847e-04 - val_loss: 9.8427e-06 - val_accuracy: 0.0000e+00\n",
      "Epoch 157/200\n",
      "18/18 [==============================] - 2s 90ms/step - loss: 3.5190e-04 - accuracy: 8.9847e-04 - val_loss: 7.1177e-06 - val_accuracy: 0.0000e+00\n",
      "Epoch 158/200\n",
      "18/18 [==============================] - 2s 96ms/step - loss: 3.4767e-04 - accuracy: 8.9847e-04 - val_loss: 1.3288e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 159/200\n",
      "18/18 [==============================] - 2s 89ms/step - loss: 3.5887e-04 - accuracy: 8.9847e-04 - val_loss: 7.4450e-06 - val_accuracy: 0.0000e+00\n",
      "Epoch 160/200\n",
      "18/18 [==============================] - 2s 89ms/step - loss: 3.6664e-04 - accuracy: 8.9847e-04 - val_loss: 1.3970e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 161/200\n",
      "18/18 [==============================] - 2s 92ms/step - loss: 3.5461e-04 - accuracy: 8.9847e-04 - val_loss: 7.1558e-06 - val_accuracy: 0.0000e+00\n",
      "Epoch 162/200\n",
      "18/18 [==============================] - 2s 89ms/step - loss: 3.3400e-04 - accuracy: 8.9847e-04 - val_loss: 7.5217e-06 - val_accuracy: 0.0000e+00\n",
      "Epoch 163/200\n",
      "18/18 [==============================] - 2s 92ms/step - loss: 3.6436e-04 - accuracy: 8.9847e-04 - val_loss: 5.6062e-06 - val_accuracy: 0.0000e+00\n",
      "Epoch 164/200\n",
      "18/18 [==============================] - 2s 86ms/step - loss: 3.3690e-04 - accuracy: 8.9847e-04 - val_loss: 5.4798e-06 - val_accuracy: 0.0000e+00\n",
      "Epoch 165/200\n",
      "18/18 [==============================] - 2s 94ms/step - loss: 3.4667e-04 - accuracy: 8.9847e-04 - val_loss: 6.1520e-06 - val_accuracy: 0.0000e+00\n",
      "Epoch 166/200\n",
      "18/18 [==============================] - 2s 84ms/step - loss: 3.3741e-04 - accuracy: 8.9847e-04 - val_loss: 1.1216e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 167/200\n",
      "18/18 [==============================] - 2s 91ms/step - loss: 4.0497e-04 - accuracy: 8.9847e-04 - val_loss: 2.4472e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 168/200\n",
      "18/18 [==============================] - 1s 74ms/step - loss: 3.8408e-04 - accuracy: 8.9847e-04 - val_loss: 7.7485e-06 - val_accuracy: 0.0000e+00\n",
      "Epoch 169/200\n",
      "18/18 [==============================] - 1s 75ms/step - loss: 4.6714e-04 - accuracy: 8.9847e-04 - val_loss: 3.8569e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 170/200\n",
      "18/18 [==============================] - 1s 73ms/step - loss: 3.9438e-04 - accuracy: 8.9847e-04 - val_loss: 5.3011e-06 - val_accuracy: 0.0000e+00\n",
      "Epoch 171/200\n",
      "18/18 [==============================] - 2s 93ms/step - loss: 3.4721e-04 - accuracy: 8.9847e-04 - val_loss: 1.2117e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 172/200\n",
      "18/18 [==============================] - 1s 73ms/step - loss: 3.5949e-04 - accuracy: 8.9847e-04 - val_loss: 5.6800e-06 - val_accuracy: 0.0000e+00\n",
      "Epoch 173/200\n",
      "18/18 [==============================] - 1s 77ms/step - loss: 4.2864e-04 - accuracy: 8.9847e-04 - val_loss: 5.1790e-06 - val_accuracy: 0.0000e+00\n",
      "Epoch 174/200\n",
      "18/18 [==============================] - 1s 76ms/step - loss: 3.7735e-04 - accuracy: 8.9847e-04 - val_loss: 9.1204e-06 - val_accuracy: 0.0000e+00\n",
      "Epoch 175/200\n",
      "18/18 [==============================] - 1s 84ms/step - loss: 3.5893e-04 - accuracy: 8.9847e-04 - val_loss: 9.5718e-06 - val_accuracy: 0.0000e+00\n",
      "Epoch 176/200\n",
      "18/18 [==============================] - 2s 87ms/step - loss: 4.2234e-04 - accuracy: 8.9847e-04 - val_loss: 1.3861e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 177/200\n",
      "18/18 [==============================] - 2s 84ms/step - loss: 3.5420e-04 - accuracy: 8.9847e-04 - val_loss: 9.5442e-06 - val_accuracy: 0.0000e+00\n",
      "Epoch 178/200\n",
      "18/18 [==============================] - 1s 82ms/step - loss: 3.4992e-04 - accuracy: 8.9847e-04 - val_loss: 6.7299e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 179/200\n",
      "18/18 [==============================] - 2s 108ms/step - loss: 3.9487e-04 - accuracy: 8.9847e-04 - val_loss: 6.2952e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 180/200\n",
      "18/18 [==============================] - 2s 106ms/step - loss: 3.2512e-04 - accuracy: 8.9847e-04 - val_loss: 5.2374e-06 - val_accuracy: 0.0000e+00\n",
      "Epoch 181/200\n",
      "18/18 [==============================] - 2s 104ms/step - loss: 3.3665e-04 - accuracy: 8.9847e-04 - val_loss: 3.5491e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 182/200\n",
      "18/18 [==============================] - 2s 97ms/step - loss: 3.1851e-04 - accuracy: 8.9847e-04 - val_loss: 1.3255e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 183/200\n",
      "18/18 [==============================] - 2s 102ms/step - loss: 3.1000e-04 - accuracy: 8.9847e-04 - val_loss: 1.1892e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 184/200\n",
      "18/18 [==============================] - 2s 94ms/step - loss: 4.0643e-04 - accuracy: 8.9847e-04 - val_loss: 5.0794e-06 - val_accuracy: 0.0000e+00\n",
      "Epoch 185/200\n",
      "18/18 [==============================] - 2s 95ms/step - loss: 3.6389e-04 - accuracy: 8.9847e-04 - val_loss: 4.6531e-06 - val_accuracy: 0.0000e+00\n",
      "Epoch 186/200\n",
      "18/18 [==============================] - 2s 108ms/step - loss: 3.2882e-04 - accuracy: 8.9847e-04 - val_loss: 6.1145e-06 - val_accuracy: 0.0000e+00\n",
      "Epoch 187/200\n",
      "18/18 [==============================] - 2s 97ms/step - loss: 3.1728e-04 - accuracy: 8.9847e-04 - val_loss: 1.4214e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 188/200\n",
      "18/18 [==============================] - 2s 84ms/step - loss: 3.0570e-04 - accuracy: 8.9847e-04 - val_loss: 1.7202e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 189/200\n",
      "18/18 [==============================] - 2s 88ms/step - loss: 2.8992e-04 - accuracy: 8.9847e-04 - val_loss: 5.1389e-06 - val_accuracy: 0.0000e+00\n",
      "Epoch 190/200\n",
      "18/18 [==============================] - 2s 88ms/step - loss: 3.2041e-04 - accuracy: 8.9847e-04 - val_loss: 8.3955e-06 - val_accuracy: 0.0000e+00\n",
      "Epoch 191/200\n",
      "18/18 [==============================] - 2s 84ms/step - loss: 3.0181e-04 - accuracy: 8.9847e-04 - val_loss: 8.5420e-06 - val_accuracy: 0.0000e+00\n",
      "Epoch 192/200\n",
      "18/18 [==============================] - 2s 105ms/step - loss: 3.4021e-04 - accuracy: 8.9847e-04 - val_loss: 3.2064e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 193/200\n",
      "18/18 [==============================] - 1s 79ms/step - loss: 3.1644e-04 - accuracy: 8.9847e-04 - val_loss: 7.1586e-06 - val_accuracy: 0.0000e+00\n",
      "Epoch 194/200\n",
      "18/18 [==============================] - 2s 96ms/step - loss: 3.0943e-04 - accuracy: 8.9847e-04 - val_loss: 4.9875e-06 - val_accuracy: 0.0000e+00\n",
      "Epoch 195/200\n",
      "18/18 [==============================] - 2s 86ms/step - loss: 3.1196e-04 - accuracy: 8.9847e-04 - val_loss: 5.2198e-06 - val_accuracy: 0.0000e+00\n",
      "Epoch 196/200\n",
      "18/18 [==============================] - 1s 71ms/step - loss: 3.5870e-04 - accuracy: 8.9847e-04 - val_loss: 6.2075e-06 - val_accuracy: 0.0000e+00\n",
      "Epoch 197/200\n",
      "18/18 [==============================] - 2s 96ms/step - loss: 3.8084e-04 - accuracy: 8.9847e-04 - val_loss: 4.9633e-06 - val_accuracy: 0.0000e+00\n",
      "Epoch 198/200\n",
      "18/18 [==============================] - 2s 85ms/step - loss: 3.0334e-04 - accuracy: 8.9847e-04 - val_loss: 4.6520e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 199/200\n",
      "18/18 [==============================] - 2s 94ms/step - loss: 3.5312e-04 - accuracy: 8.9847e-04 - val_loss: 6.0814e-06 - val_accuracy: 0.0000e+00\n",
      "Epoch 200/200\n",
      "18/18 [==============================] - 2s 106ms/step - loss: 3.2824e-04 - accuracy: 8.9847e-04 - val_loss: 2.9773e-05 - val_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x14a28f1a8d0>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train,y_train,validation_data=(X_test, y_test),epochs=200,batch_size=64,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f36ea85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_rnn_attention_model(actual, X_data):\n",
    "    y_pred = model.predict(X_data)\n",
    "    mse = mean_squared_error(actual, y_pred)\n",
    "    mae = mean_absolute_error(actual, y_pred)\n",
    "    mape = mean_absolute_percentage_error(actual, y_pred)\n",
    "    rmse = mean_squared_error(actual, y_pred, squared=False)\n",
    "    return y_pred, mse, mae, mape, rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb1c2d3",
   "metadata": {},
   "source": [
    "## 3.2 Evaluate on Validate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2ca9c826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 3s 20ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred_val, val_mse, val_mae, val_mape, val_rmse = evaluate_rnn_attention_model(y_val, X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8692c21d",
   "metadata": {},
   "source": [
    "## 3.3 Evaluate on Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a74b8287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 17ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred_test, test_mse, test_mae, test_mape, test_rmse = evaluate_rnn_attention_model(y_test, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "356c0d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35/35 [==============================] - 1s 20ms/step\n"
     ]
    }
   ],
   "source": [
    "train_predict = model.predict(X_train)\n",
    "train_predict = scaler.inverse_transform(train_predict)\n",
    "y_pred_test = scaler.inverse_transform(y_pred_test)\n",
    "y_pred_val = scaler.inverse_transform(y_pred_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "20c00184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 day input [0.02738874 0.02700352 0.02731677 0.02857191 0.02887247 0.02685112\n",
      " 0.027103   0.0246414  0.02515573 0.02420538 0.02429427 0.02404028\n",
      " 0.02396832 0.0256489  0.02551555 0.02556211 0.02546263 0.02567641\n",
      " 0.02579494 0.02602565 0.02615264 0.02629446 0.02564466 0.02598543\n",
      " 0.02744165 0.02778031 0.02761098 0.02397467 0.02485094 0.02389424\n",
      " 0.0226645  0.03015936 0.03057209 0.03090863 0.03011914 0.0299223\n",
      " 0.03059114 0.02936987 0.02893385 0.030062   0.03131714 0.03153514\n",
      " 0.0313383  0.02920054 0.03076259 0.02763003 0.02862271 0.0305213\n",
      " 0.03404754 0.03323265 0.03473544 0.02976144 0.02665428 0.026504\n",
      " 0.02379687 0.02886824 0.02923652 0.02670508 0.02692097 0.02099027\n",
      " 0.02143264 0.0239662  0.02370374 0.02391329 0.0278946  0.03085783\n",
      " 0.03068216 0.04261975 0.04341347 0.04471095 0.04476598 0.04193609\n",
      " 0.04236576 0.04330764 0.04483159 0.04291607 0.04035288 0.04101961\n",
      " 0.04179851 0.04070423 0.03937501 0.04171597 0.03963747 0.04561685\n",
      " 0.04446542 0.04933571 0.04295629 0.04176253 0.04114237 0.04237423\n",
      " 0.04960663 0.04948175 0.05249155 0.05916517 0.05749306 0.06232102\n",
      " 0.06806757 0.0687978  0.06842527 0.07064558 0.06925286 0.0712213\n",
      " 0.07359188 0.078913   0.08098091 0.08148255 0.08178945 0.08020624\n",
      " 0.08217044 0.08383197 0.07762611 0.07941675 0.0799459  0.07737635\n",
      " 0.08162859 0.08385737 0.08806304 0.08381292 0.07767903 0.07414008\n",
      " 0.07387762 0.07345219 0.07665883 0.07383952 0.08206249 0.08522257\n",
      " 0.08500033 0.08171961 0.08141693 0.08121586 0.08002421 0.08294723\n",
      " 0.07805155 0.07807271 0.074411   0.07142661 0.07584817 0.07334424\n",
      " 0.07936172 0.07852566 0.08261704 0.08016814 0.09359582 0.09577591\n",
      " 0.09618018 0.09188138 0.08733493 0.08744076 0.08552524 0.0922158\n",
      " 0.08304671 0.0869222  0.0947409  0.09330585 0.10794422 0.10918666\n",
      " 0.11025342 0.11060901 0.11112546 0.1074447  0.11329497 0.10741507\n",
      " 0.10487515 0.10457248 0.10036257 0.10223153 0.09069185 0.10305276\n",
      " 0.10645413 0.10616416 0.10977084 0.10450475 0.11818218 0.12975572\n",
      " 0.13381324 0.12973667 0.1290001  0.12385466 0.13180459 0.13163526\n",
      " 0.13614996 0.13663889 0.1270232  0.12538707 0.13558694 0.14134831\n",
      " 0.1406837  0.14185841 0.150147   0.15546812 0.14845584 0.14713297\n",
      " 0.12791429 0.12433936 0.12408748 0.12828469 0.12692795 0.11856317\n",
      " 0.12245559 0.11293091 0.13106166 0.11859704 0.11396169 0.11043968\n",
      " 0.11286318 0.10237545 0.09352174 0.08876575 0.09059237 0.09526581\n",
      " 0.08862394 0.08671054 0.08682695 0.07330403 0.07019475 0.06672353\n",
      " 0.06707489 0.0637222  0.06056848 0.06301314 0.06241203 0.07021591\n",
      " 0.06376453 0.06242685 0.06564619 0.0656102  0.06356346 0.07683874\n",
      " 0.07718586 0.08512309 0.09306455 0.09241899 0.09728081 0.09652942\n",
      " 0.10084092 0.10017208 0.09997735 0.09586269 0.09873068 0.10942371\n",
      " 0.11140061 0.11181547 0.12821484 0.12979594 0.13468104 0.12672264\n",
      " 0.13590231 0.13020233 0.14100119 0.15473789 0.16222217 0.16545844\n",
      " 0.16342228 0.16346038 0.16641303 0.1626984  0.16670935 0.16576747\n",
      " 0.16024104 0.15886102 0.16357891 0.15236307 0.15986428 0.16947574\n",
      " 0.18123766 0.17552709 0.18789646 0.18069158 0.17858768 0.17691769\n",
      " 0.16034687 0.16547749 0.15380024 0.16246558 0.16806609 0.15273136\n",
      " 0.1416785  0.14816586 0.12975784 0.15781754 0.18788588 0.17716745\n",
      " 0.20102147 0.21891304 0.20913861 0.2309607  0.24611552 0.2182188\n",
      " 0.20386829 0.20412228 0.20729717 0.19031785 0.19287893 0.20386829\n",
      " 0.22482258 0.20082039 0.19282178 0.19762433 0.19817465 0.25178799\n",
      " 0.26800111 0.27435089 0.24835911 0.22260015 0.24641184 0.25396809\n",
      " 0.22128786 0.21843046 0.19483043 0.18604021 0.18122072 0.18164616\n",
      " 0.16402974 0.14152611 0.13820517 0.12665915 0.13709396 0.1328396\n",
      " 0.13813744 0.14096309 0.13249883 0.12193279 0.12462086 0.11522318\n",
      " 0.14883048 0.15052798 0.15158417 0.14819973 0.13228082 0.12687715\n",
      " 0.12582732 0.12756716 0.1284519  0.11663707 0.09131836 0.07311353\n",
      " 0.0798189  0.07585452 0.07010373 0.06982646 0.07790339 0.08068247\n",
      " 0.07951623 0.07867383 0.07976599]\n",
      "x_input: [[0.02738874 0.02700352 0.02731677 0.02857191 0.02887247 0.02685112\n",
      "  0.027103   0.0246414  0.02515573 0.02420538 0.02429427 0.02404028\n",
      "  0.02396832 0.0256489  0.02551555 0.02556211 0.02546263 0.02567641\n",
      "  0.02579494 0.02602565 0.02615264 0.02629446 0.02564466 0.02598543\n",
      "  0.02744165 0.02778031 0.02761098 0.02397467 0.02485094 0.02389424\n",
      "  0.0226645  0.03015936 0.03057209 0.03090863 0.03011914 0.0299223\n",
      "  0.03059114 0.02936987 0.02893385 0.030062   0.03131714 0.03153514\n",
      "  0.0313383  0.02920054 0.03076259 0.02763003 0.02862271 0.0305213\n",
      "  0.03404754 0.03323265 0.03473544 0.02976144 0.02665428 0.026504\n",
      "  0.02379687 0.02886824 0.02923652 0.02670508 0.02692097 0.02099027\n",
      "  0.02143264 0.0239662  0.02370374 0.02391329 0.0278946  0.03085783\n",
      "  0.03068216 0.04261975 0.04341347 0.04471095 0.04476598 0.04193609\n",
      "  0.04236576 0.04330764 0.04483159 0.04291607 0.04035288 0.04101961\n",
      "  0.04179851 0.04070423 0.03937501 0.04171597 0.03963747 0.04561685\n",
      "  0.04446542 0.04933571 0.04295629 0.04176253 0.04114237 0.04237423\n",
      "  0.04960663 0.04948175 0.05249155 0.05916517 0.05749306 0.06232102\n",
      "  0.06806757 0.0687978  0.06842527 0.07064558 0.06925286 0.0712213\n",
      "  0.07359188 0.078913   0.08098091 0.08148255 0.08178945 0.08020624\n",
      "  0.08217044 0.08383197 0.07762611 0.07941675 0.0799459  0.07737635\n",
      "  0.08162859 0.08385737 0.08806304 0.08381292 0.07767903 0.07414008\n",
      "  0.07387762 0.07345219 0.07665883 0.07383952 0.08206249 0.08522257\n",
      "  0.08500033 0.08171961 0.08141693 0.08121586 0.08002421 0.08294723\n",
      "  0.07805155 0.07807271 0.074411   0.07142661 0.07584817 0.07334424\n",
      "  0.07936172 0.07852566 0.08261704 0.08016814 0.09359582 0.09577591\n",
      "  0.09618018 0.09188138 0.08733493 0.08744076 0.08552524 0.0922158\n",
      "  0.08304671 0.0869222  0.0947409  0.09330585 0.10794422 0.10918666\n",
      "  0.11025342 0.11060901 0.11112546 0.1074447  0.11329497 0.10741507\n",
      "  0.10487515 0.10457248 0.10036257 0.10223153 0.09069185 0.10305276\n",
      "  0.10645413 0.10616416 0.10977084 0.10450475 0.11818218 0.12975572\n",
      "  0.13381324 0.12973667 0.1290001  0.12385466 0.13180459 0.13163526\n",
      "  0.13614996 0.13663889 0.1270232  0.12538707 0.13558694 0.14134831\n",
      "  0.1406837  0.14185841 0.150147   0.15546812 0.14845584 0.14713297\n",
      "  0.12791429 0.12433936 0.12408748 0.12828469 0.12692795 0.11856317\n",
      "  0.12245559 0.11293091 0.13106166 0.11859704 0.11396169 0.11043968\n",
      "  0.11286318 0.10237545 0.09352174 0.08876575 0.09059237 0.09526581\n",
      "  0.08862394 0.08671054 0.08682695 0.07330403 0.07019475 0.06672353\n",
      "  0.06707489 0.0637222  0.06056848 0.06301314 0.06241203 0.07021591\n",
      "  0.06376453 0.06242685 0.06564619 0.0656102  0.06356346 0.07683874\n",
      "  0.07718586 0.08512309 0.09306455 0.09241899 0.09728081 0.09652942\n",
      "  0.10084092 0.10017208 0.09997735 0.09586269 0.09873068 0.10942371\n",
      "  0.11140061 0.11181547 0.12821484 0.12979594 0.13468104 0.12672264\n",
      "  0.13590231 0.13020233 0.14100119 0.15473789 0.16222217 0.16545844\n",
      "  0.16342228 0.16346038 0.16641303 0.1626984  0.16670935 0.16576747\n",
      "  0.16024104 0.15886102 0.16357891 0.15236307 0.15986428 0.16947574\n",
      "  0.18123766 0.17552709 0.18789646 0.18069158 0.17858768 0.17691769\n",
      "  0.16034687 0.16547749 0.15380024 0.16246558 0.16806609 0.15273136\n",
      "  0.1416785  0.14816586 0.12975784 0.15781754 0.18788588 0.17716745\n",
      "  0.20102147 0.21891304 0.20913861 0.2309607  0.24611552 0.2182188\n",
      "  0.20386829 0.20412228 0.20729717 0.19031785 0.19287893 0.20386829\n",
      "  0.22482258 0.20082039 0.19282178 0.19762433 0.19817465 0.25178799\n",
      "  0.26800111 0.27435089 0.24835911 0.22260015 0.24641184 0.25396809\n",
      "  0.22128786 0.21843046 0.19483043 0.18604021 0.18122072 0.18164616\n",
      "  0.16402974 0.14152611 0.13820517 0.12665915 0.13709396 0.1328396\n",
      "  0.13813744 0.14096309 0.13249883 0.12193279 0.12462086 0.11522318\n",
      "  0.14883048 0.15052798 0.15158417 0.14819973 0.13228082 0.12687715\n",
      "  0.12582732 0.12756716 0.1284519  0.11663707 0.09131836 0.07311353\n",
      "  0.0798189  0.07585452 0.07010373 0.06982646 0.07790339 0.08068247\n",
      "  0.07951623 0.07867383 0.07976599]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 345 into shape (1,142,1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[69], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m x_input\u001b[38;5;241m=\u001b[39m x_input\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx_input:\u001b[39m\u001b[38;5;124m\"\u001b[39m,x_input)\n\u001b[1;32m---> 21\u001b[0m x_input \u001b[38;5;241m=\u001b[39m \u001b[43mx_input\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m#print(x_input)\u001b[39;00m\n\u001b[0;32m     23\u001b[0m yhat \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(x_input, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 345 into shape (1,142,1)"
     ]
    }
   ],
   "source": [
    "# 13. Dá»± bÃ¡o 30 ngÃ y tiáº¿p theo\n",
    "x_input=val_data[60:].reshape(1,-1)\n",
    "x_input.shape\n",
    "\n",
    "temp_input=list(x_input)\n",
    "temp_input=temp_input[0].tolist()\n",
    "\n",
    "from numpy import array\n",
    "\n",
    "lst_output=[]\n",
    "n_steps=142\n",
    "i=0\n",
    "while(i<30):\n",
    "    \n",
    "    if(len(temp_input)>100):\n",
    "        #print(temp_input)\n",
    "        x_input=np.array(temp_input[1:])\n",
    "        print(\"{} day input {}\".format(i,x_input))\n",
    "        x_input= x_input.reshape(1,-1)\n",
    "        print(\"x_input:\",x_input)\n",
    "        x_input = x_input.reshape((1, n_steps, 1))\n",
    "        #print(x_input)\n",
    "        yhat = model.predict(x_input, verbose=0)\n",
    "        print(\"{} day output {}\".format(i,yhat))\n",
    "        temp_input.extend(yhat[0].tolist())\n",
    "        temp_input=temp_input[1:]\n",
    "        #print(temp_input)\n",
    "        lst_output.extend(yhat.tolist())\n",
    "        i=i+1\n",
    "    else:\n",
    "        x_input = x_input.reshape((1, n_steps, 1))\n",
    "        yhat = model.predict(x_input, verbose=0)\n",
    "        print(yhat[0])\n",
    "        temp_input.extend(yhat[0].tolist())\n",
    "        print(len(temp_input))\n",
    "        lst_output.extend(yhat.tolist())\n",
    "        i=i+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "86f67620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               MSE       MAE      MAPE      RMSE\n",
      "Validate  0.000125  0.007988  0.084599  0.011189\n",
      "Test      0.000030  0.005003  0.302363  0.005456\n"
     ]
    }
   ],
   "source": [
    "metrics = {\n",
    "    'MSE': [val_mse, test_mse],\n",
    "    'MAE': [val_mae, test_mae],\n",
    "    'MAPE': [val_mape, test_mape],\n",
    "    'RMSE': [val_rmse, test_rmse],\n",
    "}\n",
    "\n",
    "acc = pd.DataFrame(metrics, index=['Validate', 'Test'])\n",
    "\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a25bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14.Váº½ hÃ¬nh\n",
    "\n",
    "train_data_index = pd.RangeIndex(start=0, stop=train_size, step=1)\n",
    "plt.plot(scaler.inverse_transform(train_data))\n",
    "\n",
    "test_data_index = pd.RangeIndex(start=train_size, stop=train_size+test_size, step=1)\n",
    "plt.plot(test_data_index,scaler.inverse_transform(test_data))\n",
    "test_data_index = pd.RangeIndex(start=train_size+101, stop=train_size+test_size, step=1)\n",
    "plt.plot(test_data_index,(y_pred_test))\n",
    "\n",
    "val_data_index = pd.RangeIndex(start=train_size+test_size, stop=train_size+test_size+val_size, step=1)\n",
    "plt.plot(val_data_index,scaler.inverse_transform(val_data))\n",
    "val_data_index = pd.RangeIndex(start=train_size+test_size+101, stop=train_size+test_size+val_size, step=1)\n",
    "plt.plot(val_data_index,y_pred_val)\n",
    "\n",
    "prediect_data_index = pd.RangeIndex(start=len(df1)-1, stop=len(df1)+29, step=1)\n",
    "plt.plot(prediect_data_index,scaler.inverse_transform(lst_output))\n",
    "\n",
    "plt.legend(['Train','Test','Predict','Validate','ValidatePred','Predict30days'])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
